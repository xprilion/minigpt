{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNsVDHaTGhNFWVAb/Yi4Nx2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xprilion/minigpt/blob/main/MiniGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWYYO21xX_LI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import requests\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Hyperparameters ---\n",
        "batch_size = 32        # How many independent sequences will we process in parallel?\n",
        "                       # NOTE: If you run out of memory, try reducing this to 16.\n",
        "block_size = 256       # What is the maximum context length for predictions? (Increased from 128)\n",
        "max_iters = 5000      # How many training iterations? (Increased from 7000)\n",
        "eval_interval = 500    # How often to evaluate the model's performance?\n",
        "learning_rate = 3e-4   # A slightly lower learning rate is often better for transformers\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Use GPU if available\n",
        "eval_iters = 200       # How many batches to use for evaluation?\n",
        "n_embd = 256           # Embedding dimension (Increased from 128)\n",
        "n_head = 8             # Number of self-attention heads (Increased from 4)\n",
        "n_layer = 8            # Number of transformer blocks (Increased from 6)\n",
        "dropout = 0.2          # Dropout rate."
      ],
      "metadata": {
        "id": "1W-ZRYC2YGKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. The Dataset ---\n",
        "# We will download the 'tiny_shakespeare' dataset manually from Karpathy's repo.\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "file_path = \"input.txt\"\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    print(\"Downloading Tiny Shakespeare dataset...\")\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(requests.get(url).text)\n",
        "else:\n",
        "    print(\"Tiny Shakespeare dataset already exists locally.\")\n",
        "\n",
        "# Read the dataset\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "print(f\"Dataset loaded. Length of dataset in characters: {len(text)}\")\n",
        "\n",
        "\n",
        "# Create the vocabulary from the entire text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Create a mapping from characters to integers and vice-versa\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Create the training and validation data by splitting the raw text\n",
        "n = int(0.9 * len(text))\n",
        "train_text = text[:n]\n",
        "val_text = text[n:]\n",
        "\n",
        "# Encode the text splits into torch tensors\n",
        "train_data = torch.tensor(encode(train_text), dtype=torch.long)\n",
        "val_data = torch.tensor(encode(val_text), dtype=torch.long)\n",
        "\n",
        "\n",
        "# --- Dataset Class (remains the same) ---\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        chunk = self.data[idx:idx + self.block_size + 1]\n",
        "        x = chunk[:-1]\n",
        "        y = chunk[1:]\n",
        "        return x, y\n",
        "\n",
        "def get_dataloader(data, batch_size, block_size):\n",
        "    dataset = TextDataset(data, block_size)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkM3zU1ZYJBj",
        "outputId": "d9c3a194-d8e2-486d-dd8c-21c1dc319f4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Tiny Shakespeare dataset...\n",
            "Dataset loaded. Length of dataset in characters: 1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. The GPT Model Components (all remain the same) ---\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "QVvM2hvzYWMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Training the Model (remains the same) ---\n",
        "\n",
        "print(f\"Running on device: {device}\")\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "train_loader = get_dataloader(train_data, batch_size, block_size)\n",
        "val_loader = get_dataloader(val_data, batch_size, block_size)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        loader = train_loader if split == 'train' else val_loader\n",
        "        # Use a fresh iterator for evaluation\n",
        "        data_iter = iter(loader)\n",
        "        for k in range(eval_iters):\n",
        "            try:\n",
        "                X, Y = next(data_iter)\n",
        "            except StopIteration:\n",
        "                # This can happen if eval_iters is larger than the number of batches\n",
        "                break\n",
        "            X, Y = X.to(device), Y.to(device)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses[losses != 0].mean() # Exclude zero losses if StopIteration occurred\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "print(\"Starting training...\")\n",
        "train_iter = iter(train_loader)\n",
        "for iter_num in range(max_iters):\n",
        "    if iter_num % eval_interval == 0 or iter_num == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    try:\n",
        "        xb, yb = next(train_iter)\n",
        "    except StopIteration:\n",
        "        train_iter = iter(train_loader)\n",
        "        xb, yb = next(train_iter)\n",
        "\n",
        "    xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"Training finished.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dnvHHLkYc2B",
        "outputId": "e958f5b0-b6d9-4b30-c035-f8e7fcae43ed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device: cuda\n",
            "Starting training...\n",
            "step 0: train loss 4.3730, val loss 4.3749\n",
            "step 500: train loss 2.2684, val loss 2.2935\n",
            "step 1000: train loss 1.8958, val loss 1.9992\n",
            "step 1500: train loss 1.6860, val loss 1.8364\n",
            "step 2000: train loss 1.5654, val loss 1.7528\n",
            "step 2500: train loss 1.4755, val loss 1.6894\n",
            "step 3000: train loss 1.4207, val loss 1.6326\n",
            "step 3500: train loss 1.3791, val loss 1.5961\n",
            "step 4000: train loss 1.3398, val loss 1.5670\n",
            "step 4500: train loss 1.3142, val loss 1.5479\n",
            "step 4999: train loss 1.2872, val loss 1.5404\n",
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 4. Generate from the Model ---\n",
        "print(\"\\n--- Generating Text ---\")\n",
        "start_string = \"LADY CAPULET: \\n Nurse, where's my daughter?\"\n",
        "context = torch.tensor(encode(start_string), dtype=torch.long, device=device).unsqueeze(0)\n",
        "generated_sequence = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
        "print(f\"Generated sequence starting with '{start_string}':\")\n",
        "print(generated_sequence)"
      ],
      "metadata": {
        "id": "ooUphqT5YkcN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faced502-34a8-4971-9d95-b641f910d1d9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Generating Text ---\n",
            "Generated sequence starting with 'LADY CAPULET: \n",
            " Nurse, where's my daughter?':\n",
            "LADY CAPULET: \n",
            " Nurse, where's my daughter? \n",
            "My love my birg: what I heard he baniness\n",
            "All Maria Edwas close\n",
            "The ext. I thristers of from king.\n",
            "\n",
            "ROMEO:\n",
            "Basholing I go my has head, I would to you the year\n",
            "Friend, content. Your hurself state, doth look, and them\n",
            "To being the spit and people; &thall greet thee, who\n",
            "Feen the shabband, tell thee that no morter wastioner, and\n",
            "tis whilst we dead? A' fall touch often o' the city's face\n",
            "with her tanchital and thy king wors?\n",
            "\n",
            "LEONTES:\n",
            "No, good devill thee, sleep?\n",
            "\n",
            "Second Murderer:\n",
            "So long methouth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QZCTiY3uaXpR"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}